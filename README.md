# Twitter Sentiment Classification: LSTM + TFâ€‘IDF vs BERT

Endâ€‘toâ€‘end comparison of a classic **LSTM + TFâ€‘IDF** text classifier and a fineâ€‘tuned **BERT Transformer** on a **4â€‘class Twitter sentiment dataset**: Irrelevant, Negative, Neutral, Positive.

## ğŸ“Š Results

On the heldâ€‘out Twitter validation set:

| Model         | Accuracy | F1â€‘Score |
|--------------|----------|----------|
| LSTM + TFâ€‘IDF| 0.9520   | 0.9520   |
| **BERT**     | **0.9640** | **0.9640** |

BERT achieves slightly higher accuracy/F1 and fewer misclassifications, while LSTM + TFâ€‘IDF remains a strong and lighter baseline.

## ğŸ§  Models

- **LSTM + TFâ€‘IDF**
  - TFâ€‘IDF vectorizer converts tweets to sparse vectors.
  - Single LSTM layer consumes the vector (reshaped to sequence) and outputs class probabilities.
  - Saved as `best_lstm_model.h5` with `tfidf_vectorizer.pkl` and `label_encoder.pkl`.

- **BERT**
  - Fineâ€‘tuned `AutoModelForSequenceClassification` from Hugging Face.
  - Uses subword tokenization, attention and contextual embeddings.
  - Stored in a directory like `bert_sentiment_model/` containing config, tokenizer, and weights.

Both models predict the same 4 sentiment labels encoded by the shared `LabelEncoder`.

## ğŸ—‚ï¸ Project Structure

```text
twitter-sentiment/
â”œâ”€â”€ model_comparison.py        # LSTM vs BERT evaluation + plots
â”œâ”€â”€ best_lstm_model.h5         # Trained LSTM model
â”œâ”€â”€ tfidf_vectorizer.pkl       # TFâ€‘IDF vectorizer for LSTM
â”œâ”€â”€ label_encoder.pkl          # Class encoder (4 labels)
â”œâ”€â”€ bert_sentiment_model/      # Fineâ€‘tuned BERT checkpoint
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â””â”€â”€ tokenizer_config.json
â”œâ”€â”€ twitter_validation.csv     # Validation set: id, platform, label, text
â””â”€â”€ README.md
```

Recommended `.gitignore` (if you put this on GitHub):

```gitignore
*.h5
*.pkl
bert_sentiment_model/
*.csv
__pycache__/
*.pyc
```

## ğŸš€ How to Run

1. Install dependencies (example):

```bash
pip install tensorflow torch transformers scikit-learn pandas numpy matplotlib seaborn
```

2. Place the following files in the project folder:

- `best_lstm_model.h5`
- `tfidf_vectorizer.pkl`
- `label_encoder.pkl`
- `bert_sentiment_model/` (folder)
- `twitter_validation.csv`

3. Run the comparison script:

```bash
python model_comparison.py
```

This will:

- Load the validation tweets.
- Run predictions with:
  - `predict_lstm()` (TFâ€‘IDF â†’ LSTM)
  - `predict_bert()` (tokenizer â†’ BERT)
- Print the metrics table above.
- Show:
  - Bar chart of **Accuracy & F1â€‘Score**
  - Confusion matrices for both models
  - Bar chart of misclassification counts
  - A text list of the top disagreements between LSTM and BERT.

## ğŸ” Interpretation

- **LSTM + TFâ€‘IDF** already reaches **95.2%** accuracy â†’ great strong baseline.
- **BERT** pushes performance to **96.4%** and reduces errors in all four sentiment classes.
- Confusion matrices show that both models are best on Neutral/Positive, with BERT slightly better at avoiding crossâ€‘class confusion (e.g. Negative vs Neutral).

